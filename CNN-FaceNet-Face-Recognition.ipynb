{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN-FaceNet-Face-Recognition.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# CNN-FaceNet-Face-Recognition\n","\n","Welcome to a new CNN project!\n","\n","In this project, we are going to use pre-trained model [FaceNet](https://arxiv.org/pdf/1503.03832.pdf) to solve two common face recognition problems:\n","- **Face Verification** \"Is this the claimed person?\" For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.\n","\n","- **Face Recognition** \"Who is this person?\" For example, the video lecture showed a [face recognition video](https://www.youtube.com/watch?v=wr4rx0Spihs) of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l5FfBGV5yUjb"},"source":["## Table of Contents\n","- [1 - Set up the working directory & Import packages ](#1)\n","- [2 - Load pre-trained model ](#2)\n","- [3 - Build the database of the encoded images](#3)\n","- [4 - Apply the model for Face Verification](#4)\n","- [5 - Apply the model for Face Regconition](#5)\n"]},{"cell_type":"markdown","metadata":{"id":"9brUxyTpYZHy"},"source":["<a name='1'></a>\n","## 1 - Set up the working directory & Import packages ##"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQsjMdZZcV5E","executionInfo":{"status":"ok","timestamp":1635403124651,"user_tz":300,"elapsed":324,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"d5ed9164-2715-4671-a192-19579cb7b69f"},"source":["# Move to the working directory on Google Drive as using Google Colab\n","import os\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  PROJECT_ROOT =\"/content/drive/MyDrive/GitHub/CNN-FaceNet-Face-Recognition\"\n","  os.chdir(PROJECT_ROOT)\n","  !pwd\n","else:\n","  PROJECT_ROOT =\".\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on CoLab\n","/content/drive/MyDrive/GitHub/CNN-FaceNet-Face-Recognition\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8XvNWe7niky","executionInfo":{"status":"ok","timestamp":1635403124652,"user_tz":300,"elapsed":6,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"f14fd22a-d96b-4a68-d064-17bdacac2042"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hw_KrXDLSyAT","executionInfo":{"status":"ok","timestamp":1635403125180,"user_tz":300,"elapsed":531,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"9c5b0666-b48b-4ea6-80e7-8ecdfe18b21a"},"source":["# View Nvidia CUDA drivers\n","!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"cyhokC5kgarp","executionInfo":{"status":"ok","timestamp":1635403127616,"user_tz":300,"elapsed":2439,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["from tensorflow.keras.models import model_from_json\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.layers import Lambda, Flatten, Dense\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import backend as K\n","K.set_image_data_format('channels_last')\n","import os\n","import numpy as np\n","from numpy import genfromtxt\n","import pandas as pd\n","import tensorflow as tf\n","import PIL\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1sOvblBKdHEr"},"source":["<a name='2'></a>\n","## 2 - Load Pre-trained model to Encode a Face Image into a 128-Dimensional Vector##\n"]},{"cell_type":"code","metadata":{"id":"NR_M9nWN-K8B","executionInfo":{"status":"ok","timestamp":1635403140227,"user_tz":300,"elapsed":12615,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["# Load the model\n","json_file = open(PROJECT_ROOT + '/keras-facenet-h5/model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(loaded_model_json)\n","model.load_weights(PROJECT_ROOT + '/keras-facenet-h5/model.h5')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClebU9NJg99G","executionInfo":{"status":"ok","timestamp":1635403140228,"user_tz":300,"elapsed":16,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["# Map each person's name to a 128-dimensional encoding of their face.\n","def img_to_encoding(image_path, model):\n","    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n","    img = np.around(np.array(img) / 255.0, decimals=12)\n","    x_train = np.expand_dims(img, axis=0)\n","    embedding = model.predict_on_batch(x_train)\n","    return embedding / np.linalg.norm(embedding, ord=2)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sn9m9D3UimHM"},"source":["<a name='3'></a>\n","## 3 - Build the database of the encoded images ##\n"]},{"cell_type":"code","metadata":{"id":"PHVhoCJYriry","executionInfo":{"status":"ok","timestamp":1635403214416,"user_tz":300,"elapsed":6764,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["# build the database\n","database = {}\n","database[\"younes\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/younes.jpg\", model)\n","database[\"tian\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/tian.jpg\", model)\n","database[\"andrew\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/andrew.jpg\", model)\n","database[\"kian\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/kian.jpg\", model)\n","database[\"dan\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/dan.jpg\", model)\n","database[\"sebastiano\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/sebastiano.jpg\", model)\n","database[\"bertrand\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/bertrand.jpg\", model)\n","database[\"kevin\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/kevin.jpg\", model)\n","database[\"felix\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/felix.jpg\", model)\n","database[\"benoit\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/benoit.jpg\", model)\n","database[\"arnaud\"] = img_to_encoding(PROJECT_ROOT + \"/images_orig/arnaud.jpg\", model)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5Boql560C5k"},"source":["<a name='4'></a>\n","## 4 - Apply the model for Face Verification ##"]},{"cell_type":"markdown","metadata":{"id":"fqrP9TPn007z"},"source":[" Build a function that verifies if the person on the \"image_path\" image is \"identity\"."]},{"cell_type":"code","metadata":{"id":"60wtPUYkqp93","executionInfo":{"status":"ok","timestamp":1635403219243,"user_tz":300,"elapsed":5,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["def verify(image_path, identity, database, model,dist_thres=1.0):\n","    \"\"\"\n","    Function that verifies if the person on the \"image_path\" image is \"identity\".\n","    \n","    Arguments:\n","        image_path -- path to an image\n","        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n","        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n","        model -- your Inception model instance in Keras\n","    \n","    Returns:\n","        dist -- distance between the image_path and the image of \"identity\" in the database.\n","        door_open -- True, if the door should open. False otherwise.\n","    \"\"\"\n","    ### START CODE HERE\n","    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n","    encoding = img_to_encoding(image_path, model)\n","    # Step 2: Compute distance with identity's image (≈ 1 line)\n","    dist = np.linalg.norm(encoding-database[identity])\n","    # Step 3: Open the door if dist < 1.0, else don't open (≈ 3 lines)\n","    if dist < dist_thres:\n","        print(\"Correct! It's \" + str(identity))\n","        door_open = True\n","    else:\n","        print(\"Incorrect! It's not \" + str(identity))\n","        door_open = False\n","    ### END CODE HERE        \n","    return dist, door_open"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zM7lV59d1R0y"},"source":["Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nw88Y5nrCq5","executionInfo":{"status":"ok","timestamp":1635403239070,"user_tz":300,"elapsed":664,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"c8806e00-d82b-47c5-ec84-9837b937555d"},"source":["recorded_person = PROJECT_ROOT + \"/images_tested/camera_0.jpg\"\n","verify(recorded_person, \"younes\", database, model)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! It's younes\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.59929454, True)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"q4cHT_ao2z-T"},"source":["<a name='5'></a>\n","## 5 - Apply the model for Face Regconition##"]},{"cell_type":"markdown","metadata":{"id":"QZVPTitg3c-j"},"source":[" Build a function that finds who is the person on the image_path image."]},{"cell_type":"code","metadata":{"id":"z_SPfmiP3CJp","executionInfo":{"status":"ok","timestamp":1635403252670,"user_tz":300,"elapsed":201,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}}},"source":["def who_is_it(image_path, database, model,dist_thres=1.0):\n","    \"\"\"\n","    Implements face recognition for the office by finding who is the person on the image_path image.\n","    \n","    Arguments:\n","        image_path -- path to an image\n","        database -- database containing image encodings along with the name of the person on the image\n","        model -- your Inception model instance in Keras\n","    \n","    Returns:\n","        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n","        identity -- string, the name prediction for the person on image_path\n","    \"\"\"\n","    \n","    ### START CODE HERE\n","\n","    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n","    encoding =  img_to_encoding(image_path, model)\n","    \n","    ## Step 2: Find the closest encoding ##\n","    \n","    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n","    min_dist = 100\n","    \n","    # Loop over the database dictionary's names and encodings.\n","    for (name, db_enc) in database.items():\n","        \n","        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n","        dist = np.linalg.norm(encoding-db_enc)\n","\n","        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n","        if dist<min_dist:\n","            min_dist = dist\n","            identity = name\n","    ### END CODE HERE\n","    \n","    if min_dist > dist_thres:\n","        print(\"Not in the database.\")\n","    else:\n","        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n","        \n","    return min_dist, identity"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2oiUOdyA3opK"},"source":[" Let's see if our who_it_is() algorithm identifies a person in the database."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCQZSSlS3uQZ","executionInfo":{"status":"ok","timestamp":1635403254963,"user_tz":300,"elapsed":391,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"60527354-c853-4ffb-c103-565bab0cb8ad"},"source":["# Test 1 with Younes pictures \n","recorded_person = PROJECT_ROOT + \"/images_tested/camera_0.jpg\"\n","who_is_it(recorded_person, database, model)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["it's younes, the distance is 0.59929454\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.59929454, 'younes')"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-txd-XgoUpo","executionInfo":{"status":"ok","timestamp":1635403314105,"user_tz":300,"elapsed":385,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"66d184a8-c284-42c8-923b-16972377f157"},"source":["image_path = 'images_orig/younes.jpg'\n","encoding = img_to_encoding(image_path, model)\n","encoding"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.04321312,  0.02769636, -0.11006935, -0.02181004,  0.0361076 ,\n","        -0.0456748 ,  0.01104234,  0.01584502, -0.06034191, -0.06126551,\n","         0.03670533, -0.0961697 ,  0.10318674, -0.0190394 ,  0.12444156,\n","         0.14966859, -0.00199561, -0.15202133, -0.08695692,  0.1351084 ,\n","         0.09610432,  0.01601146, -0.04747052,  0.09019998, -0.00961677,\n","        -0.05942307,  0.02899263, -0.02146621, -0.15220144,  0.03687984,\n","         0.04161033, -0.03324274,  0.03212211,  0.15484709, -0.03253124,\n","         0.13734728, -0.03606853, -0.19555251, -0.03386902,  0.138185  ,\n","         0.01098225,  0.05590543,  0.07494756,  0.1048388 ,  0.04818638,\n","        -0.08864869,  0.18727283,  0.09609108,  0.07169706,  0.00777906,\n","        -0.1526765 ,  0.02436365, -0.13910365,  0.00391132, -0.00524655,\n","        -0.14967903, -0.12527572,  0.1339501 ,  0.12619701,  0.12007786,\n","         0.00050071, -0.0457189 , -0.02791403, -0.07097332,  0.0454842 ,\n","        -0.08937535, -0.10754488,  0.07807683, -0.03811925, -0.00151575,\n","         0.01220499, -0.05128307,  0.03364238, -0.04656569, -0.0769327 ,\n","         0.01517426,  0.12072707,  0.03137715, -0.01364711,  0.03496314,\n","        -0.11779601, -0.06153864,  0.05707043,  0.12661038,  0.09578301,\n","         0.03803815, -0.03495955, -0.12204701,  0.02412389, -0.11608056,\n","         0.14219458,  0.03719082, -0.00265482, -0.05039633, -0.06154341,\n","         0.08947042, -0.16554719, -0.0709751 , -0.15957746,  0.15339436,\n","         0.04463784, -0.00404566, -0.08388822,  0.00045185, -0.16575852,\n","        -0.1775658 ,  0.01101201,  0.02186297, -0.1126481 ,  0.07773103,\n","        -0.02995123, -0.11883248,  0.19866861,  0.03217371,  0.0251146 ,\n","        -0.07490955,  0.08095314, -0.05114298,  0.01205273,  0.05933284,\n","         0.15104261,  0.01442059, -0.02418399, -0.11646814, -0.04839946,\n","        -0.1634367 ,  0.09713511,  0.00650206]], dtype=float32)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fe2CiBrOojF9","executionInfo":{"status":"ok","timestamp":1635403401494,"user_tz":300,"elapsed":166,"user":{"displayName":"Daniel Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01562824883738723040"}},"outputId":"3a30fc47-3d68-438c-c0db-43b9c50fcf2e"},"source":["img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n","img = np.around(np.array(img) / 255.0, decimals=12)\n","img"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[1.        , 1.        , 0.70588235],\n","        [1.        , 1.        , 0.70588235],\n","        [0.95686275, 0.94901961, 0.76078431],\n","        ...,\n","        [0.74901961, 0.82352941, 0.56470588],\n","        [0.54117647, 0.62352941, 0.36862745],\n","        [0.54117647, 0.62352941, 0.36862745]],\n","\n","       [[1.        , 1.        , 0.70588235],\n","        [1.        , 1.        , 0.70588235],\n","        [0.95686275, 0.94901961, 0.76078431],\n","        ...,\n","        [0.74901961, 0.82352941, 0.56470588],\n","        [0.54117647, 0.62352941, 0.36862745],\n","        [0.54117647, 0.62352941, 0.36862745]],\n","\n","       [[0.98431373, 0.97647059, 0.7372549 ],\n","        [0.98431373, 0.97647059, 0.7372549 ],\n","        [1.        , 1.        , 0.84313725],\n","        ...,\n","        [0.65882353, 0.70196078, 0.46666667],\n","        [0.69411765, 0.74901961, 0.50980392],\n","        [0.69411765, 0.74901961, 0.50980392]],\n","\n","       ...,\n","\n","       [[0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        ...,\n","        [0.25882353, 0.25098039, 0.25490196],\n","        [0.26666667, 0.26666667, 0.26666667],\n","        [0.26666667, 0.26666667, 0.26666667]],\n","\n","       [[0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        ...,\n","        [0.25490196, 0.24705882, 0.25098039],\n","        [0.2627451 , 0.2627451 , 0.2627451 ],\n","        [0.2627451 , 0.2627451 , 0.2627451 ]],\n","\n","       [[0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        [0.99607843, 0.99607843, 0.98823529],\n","        ...,\n","        [0.25490196, 0.24705882, 0.25098039],\n","        [0.2627451 , 0.2627451 , 0.2627451 ],\n","        [0.2627451 , 0.2627451 , 0.2627451 ]]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"X6lPRoUKoyfr"},"source":[""],"execution_count":null,"outputs":[]}]}